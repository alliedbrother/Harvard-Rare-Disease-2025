{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Modules and Mock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from lxml import etree\n",
    "\n",
    "def parse_newstyle_orphanet(xml_path):\n",
    "    \"\"\"\n",
    "    Parses an Orphanet XML with structure like:\n",
    "    <JDBOR>\n",
    "      <HPODisorderSetStatusList>\n",
    "        <HPODisorderSetStatus>\n",
    "          <Disorder>\n",
    "            <OrphaCode>...</OrphaCode>\n",
    "            <Name>...</Name>\n",
    "            <HPODisorderAssociationList>...</HPODisorderAssociationList>\n",
    "          </Disorder>\n",
    "        </HPODisorderSetStatus>\n",
    "        ...\n",
    "      </HPODisorderSetStatusList>\n",
    "    </JDBOR>\n",
    "    \n",
    "    Returns a dict:\n",
    "    {\n",
    "      \"ORPHA:58 | Alexander disease\": {\n",
    "        \"hpo_terms\": [...],\n",
    "        \"frequencies\": { \"HP:0000256\": \"Very frequent (99-80%)\", ... }\n",
    "      },\n",
    "      ...\n",
    "    }\n",
    "    \"\"\"\n",
    "    tree = etree.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    disease_dict = {}\n",
    "\n",
    "    # Grab all <Disorder> elements from any depth\n",
    "    # e.g. //Disorder means \"all Disorder tags at any level\"\n",
    "    disorders = root.xpath(\"//Disorder\")\n",
    "\n",
    "    for disorder in disorders:\n",
    "        # 1) OrphaCode\n",
    "        orpha_code_el = disorder.find(\"OrphaCode\")\n",
    "        if orpha_code_el is not None:\n",
    "            orpha_code = orpha_code_el.text\n",
    "        else:\n",
    "            continue  # skip if we can't find OrphaCode\n",
    "\n",
    "        # 2) Disease name\n",
    "        name_el = disorder.find(\"Name\")\n",
    "        if name_el is not None:\n",
    "            disease_name = name_el.text\n",
    "        else:\n",
    "            disease_name = f\"UnknownOrpha_{orpha_code}\"\n",
    "\n",
    "        disease_key = f\"ORPHA:{orpha_code} | {disease_name}\"\n",
    "\n",
    "        # 3) HPO associations\n",
    "        hpo_terms = []\n",
    "        freq_map = {}\n",
    "\n",
    "        assoc_list_el = disorder.find(\"HPODisorderAssociationList\")\n",
    "        if assoc_list_el is not None:\n",
    "            associations = assoc_list_el.findall(\"HPODisorderAssociation\")\n",
    "            for assoc in associations:\n",
    "                hpo_el = assoc.find(\"HPO\")\n",
    "                if hpo_el is not None:\n",
    "                    hpo_id_el = hpo_el.find(\"HPOId\")\n",
    "                    if hpo_id_el is not None:\n",
    "                        hpo_id = hpo_id_el.text\n",
    "                    else:\n",
    "                        hpo_id = None\n",
    "\n",
    "                    # frequency\n",
    "                    freq_el = assoc.find(\"HPOFrequency\")\n",
    "                    if freq_el is not None:\n",
    "                        freq_name_el = freq_el.find(\"Name\")\n",
    "                        if freq_name_el is not None:\n",
    "                            freq_name = freq_name_el.text\n",
    "                        else:\n",
    "                            freq_name = None\n",
    "                    else:\n",
    "                        freq_name = None\n",
    "\n",
    "                    if hpo_id:\n",
    "                        hpo_terms.append(hpo_id)\n",
    "                        if freq_name:\n",
    "                            freq_map[hpo_id] = freq_name\n",
    "\n",
    "        # remove duplicates\n",
    "        hpo_terms = list(set(hpo_terms))\n",
    "\n",
    "        disease_dict[disease_key] = {\n",
    "            \"hpo_terms\": hpo_terms,\n",
    "            \"frequencies\": freq_map\n",
    "        }\n",
    "\n",
    "    return disease_dict\n",
    "\n",
    "\n",
    "def load_orphanet_data(json_path, xml_path):\n",
    "    \"\"\"\n",
    "    If json_path exists, load from it.\n",
    "    Otherwise parse the XML, save the result to json_path, then return it.\n",
    "    \"\"\"\n",
    "    if os.path.exists(json_path):\n",
    "        # load from JSON\n",
    "        with open(json_path, \"r\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    else:\n",
    "        # parse from XML\n",
    "        data = parse_newstyle_orphanet(xml_path)\n",
    "        with open(json_path, \"w\") as f:\n",
    "            json.dump(data, f, indent=2)\n",
    "        return data\n",
    "\n",
    "\n",
    "# Test function call (for a .ipynb cell)\n",
    "def test_orphanet_parser():\n",
    "    xml_file = \"data/en_product6.xml\"        # replace with your file\n",
    "    json_file = \"data/converted_orphanet.json\"   # output\n",
    "    data = load_orphanet_data(json_file, xml_file)\n",
    "    print(f\"Parsed {len(data)} diseases.\")\n",
    "    # Print first 3 entries\n",
    "    keys = list(data.keys())[:3]\n",
    "    for k in keys:\n",
    "        print(k, data[k])\n",
    "\n",
    "# Usage:\n",
    "# test_orphanet_parser()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_orphanet_parser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def load_hpo_terms(file_path):\n",
    "    \"\"\"\n",
    "    Reads a file of HPO terms in the format: HP:NNNNNNN <tab> PhenotypeName\n",
    "    Returns a dict: { \"phenotypename\".lower(): \"HP:NNNNNNN\" }\n",
    "    \"\"\"\n",
    "    hpo_dict = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                hpo_id, phenotype_name = parts\n",
    "                hpo_dict[phenotype_name.lower()] = hpo_id\n",
    "    return hpo_dict\n",
    "\n",
    "def load_synonyms(file_path):\n",
    "    \"\"\"\n",
    "    Reads synonyms in the format: Synonym <tab> HP:NNNNNNN\n",
    "    Returns a dict: { \"synonym\".lower(): \"HP:NNNNNNN\" }\n",
    "    \"\"\"\n",
    "    synonym_dict = {}\n",
    "    with open(file_path, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(\"\\t\")\n",
    "            if len(parts) == 2:\n",
    "                synonym, hpo_id = parts\n",
    "                synonym_dict[synonym.lower()] = hpo_id\n",
    "    return synonym_dict\n",
    "\n",
    "def extract_hpo_terms_from_text(text, hpo_dict, synonym_dict):\n",
    "    \"\"\"\n",
    "    Given free-text clinical description, find matching HPO IDs by substring checks.\n",
    "    Returns a list of (hpo_id, matched_string).\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    matched_terms = []\n",
    "\n",
    "    # Direct phenotype name matches\n",
    "    for phenotype_name, hpo_id in hpo_dict.items():\n",
    "        if phenotype_name in text:\n",
    "            matched_terms.append((hpo_id, phenotype_name))\n",
    "\n",
    "    # Synonym matches\n",
    "    for synonym, hpo_id in synonym_dict.items():\n",
    "        if synonym in text:\n",
    "            matched_terms.append((hpo_id, synonym))\n",
    "\n",
    "    return matched_terms\n",
    "\n",
    "def run_custom_extractor(text, terms_file=\"data/hpo_term_names.txt\", synonyms_file=\"data/hpo_synonyms.txt\"):\n",
    "    \"\"\"\n",
    "    High-level function: \n",
    "      1) Load HPO terms & synonyms\n",
    "      2) Extract matches from 'text'\n",
    "      3) Return unique list of HPO IDs\n",
    "    \"\"\"\n",
    "    hpo_dict = load_hpo_terms(terms_file)\n",
    "    synonym_dict = load_synonyms(synonyms_file)\n",
    "    matches = extract_hpo_terms_from_text(text, hpo_dict, synonym_dict)\n",
    "    unique_ids = list({m[0] for m in matches})\n",
    "    return unique_ids\n",
    "\n",
    "# Test function call\n",
    "def test_custom_extractor():\n",
    "    sample_text = \"Patient has severe headache and occasional macrocephaly\"\n",
    "    # Adjust the file paths as needed\n",
    "    hpo_ids = run_custom_extractor(sample_text)\n",
    "    print(\"Text:\", sample_text)\n",
    "    print(\"Extracted HPO IDs:\", hpo_ids)\n",
    "\n",
    "# Usage in .ipynb:\n",
    "# test_custom_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Patient has severe headache and occasional macrocephaly\n",
      "Extracted HPO IDs: ['HP:0002315', 'HP:0040283', 'HP:0000256', 'HP:0012828']\n"
     ]
    }
   ],
   "source": [
    "test_custom_extractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsed 17755 edges from data/hp.obo and wrote them to data/hp_dag.txt\n"
     ]
    }
   ],
   "source": [
    "def parse_obo_to_dag(obo_path, dag_path):\n",
    "    \"\"\"\n",
    "    Converts a basic .obo file into a DAG file of child->parent edges\n",
    "    based on 'is_a:' lines.\n",
    "    \n",
    "    :param obo_path: Path to the input .obo file (e.g., 'hp.obo')\n",
    "    :param dag_path: Path to the output DAG file (e.g., 'hp_dag.txt')\n",
    "    \n",
    "    The DAG file will contain lines like:\n",
    "       HP:0000002 HP:0001507\n",
    "       HP:0000003 HP:0000107\n",
    "       ...\n",
    "    \n",
    "    where each line is (child_id parent_id).\n",
    "    \"\"\"\n",
    "    edges = []\n",
    "    current_id = None  # Will hold the ID of the term we're currently parsing\n",
    "\n",
    "    with open(obo_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "\n",
    "            # Start of a new [Term] means we reset current_id\n",
    "            if line.startswith(\"[Term]\"):\n",
    "                current_id = None\n",
    "                continue\n",
    "\n",
    "            # If we see \"id: HP:xxx\", store that as the current term ID\n",
    "            # e.g. \"id: HP:0000003\"\n",
    "            if line.startswith(\"id: \"):\n",
    "                # Extract everything after \"id: \"\n",
    "                current_id = line.split(\"id: \")[1].strip()\n",
    "                continue\n",
    "\n",
    "            # If we see \"is_a: HP:xxx ! comment\"\n",
    "            # e.g. \"is_a: HP:0000107 ! Renal cyst\"\n",
    "            if line.startswith(\"is_a: \"):\n",
    "                # Something like \"HP:0000107 ! Renal cyst\"\n",
    "                # We'll split by spaces and take the first chunk as the parent ID\n",
    "                # line would look like: \"is_a: HP:0000107 ! Renal cyst\"\n",
    "                parent_part = line.replace(\"is_a: \", \"\")  # \"HP:0000107 ! Renal cyst\"\n",
    "                parent_part = parent_part.split(\" \")[0]  # \"HP:0000107\"\n",
    "                parent_id = parent_part.strip()\n",
    "\n",
    "                # Only store edge if we have a current_id\n",
    "                if current_id and parent_id:\n",
    "                    edges.append((current_id, parent_id))\n",
    "\n",
    "    # Now write out edges to the DAG file\n",
    "    with open(dag_path, \"w\", encoding=\"utf-8\") as out:\n",
    "        for (child_id, parent_id) in edges:\n",
    "            out.write(f\"{child_id} {parent_id}\\n\")\n",
    "\n",
    "    print(f\"Parsed {len(edges)} edges from {obo_path} and wrote them to {dag_path}\")\n",
    "\n",
    "\n",
    "def test_obo_to_dag():\n",
    "    \"\"\"\n",
    "    Simple test function that:\n",
    "      1. Reads 'hp.obo'\n",
    "      2. Writes 'hp_dag.txt'\n",
    "    Adjust paths as needed.\n",
    "    \"\"\"\n",
    "    obo_file = \"data/hp.obo\"       # Path to your .obo file\n",
    "    dag_file = \"data/hp_dag.txt\"   # Output DAG file\n",
    "\n",
    "    parse_obo_to_dag(obo_file, dag_file)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test_obo_to_dag()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genomic-diagnostics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
